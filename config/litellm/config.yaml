# LiteLLM Proxy Configuration
# This file configures the LiteLLM proxy server

# General Settings
general_settings:
  # Master key for API access
  master_key: "os.environ/MASTER_KEY"
  
  # Database configuration
  database_url: "os.environ/DATABASE_URL"
  
  # Logging configuration
  log_level: "os.environ/LITELLM_LOG_LEVEL"
  
  # Production optimizations
  set_verbose: false
  json_logs: true
  
  # Cache configuration
  cache_enabled: "false"
  
  # Production database handling
  allow_requests_on_db_unavailable: true

litellm_settings:
  callbacks: ["openmeter"] # ðŸ‘ˆ KEY CHANGE
  
# Model Configuration
# Define your models here - uncomment and configure as needed
model_list:
  # OpenAI Models
  - model_name: gpt-4.1-nano
    litellm_params:
      model: openai/gpt-4.1-nano
      api_key: "os.environ/OPENAI_API_KEY"
  
  - model_name: gpt-4.1
    litellm_params:
      model: openai/gpt-4.1
      api_key: "os.environ/OPENAI_API_KEY"

  # OpenAI Whisper Models
  - model_name: whisper-1
    litellm_params:
      model: openai/whisper-1
      api_key: os.environ/OPENAI_API_KEY
    model_info:
      mode: audio_transcription

  # - model_name: gpt-3.5-turbo
  #   litellm_params:
  #     model: gpt-3.5-turbo
  #     api_key: "os.environ/OPENAI_API_KEY"
  
  # Anthropic Models
  # - model_name: claude-3-opus
  #   litellm_params:
  #     model: claude-3-opus-20240229
  #     api_key: "os.environ/ANTHROPIC_API_KEY"
  
  # - model_name: claude-3-sonnet
  #   litellm_params:
  #     model: claude-3-sonnet-20240229
  #     api_key: "os.environ/ANTHROPIC_API_KEY"
  
  # Azure OpenAI Models
  # - model_name: azure-gpt-4
  #   litellm_params:
  #     model: azure/gpt-4
  #     api_key: "os.environ/AZURE_OPENAI_API_KEY"
  #     api_base: "os.environ/AZURE_OPENAI_API_BASE"
  #     api_version: "os.environ/AZURE_OPENAI_API_VERSION"
  
  # Google Vertex AI Models
  # - model_name: vertex-gemini-pro
  #   litellm_params:
  #     model: vertex_ai/gemini-pro
  #     vertex_project: "os.environ/GOOGLE_CLOUD_PROJECT"
  #     vertex_location: "os.environ/GOOGLE_CLOUD_LOCATION"
  
  # Cohere Models
  # - model_name: cohere-command
  #   litellm_params:
  #     model: cohere/command
  #     api_key: "os.environ/COHERE_API_KEY"

# Authentication Settings
auth_settings:
  # Authentication mode: "basic", "jwt", "custom"
  auth_mode: "os.environ/LITELLM_AUTH_MODE"
  
  # Custom authentication callback URL
  auth_callback: "os.environ/LITELLM_AUTH_CALLBACK"

# Rate Limiting Settings
rate_limiting:
  enabled: "os.environ/LITELLM_RATE_LIMITING_ENABLED"
  strategy: "os.environ/LITELLM_RATE_LIMITING_STRATEGY"
  
  # Rate limits per user
  user_rate_limits:
    default: "100/minute"
    premium: "1000/minute"

# Monitoring Settings
monitoring:
  enabled: "os.environ/LITELLM_MONITORING_ENABLED"
  alerting_enabled: "os.environ/LITELLM_MONITORING_ALERTING_ENABLED"
  
  # Metrics collection
  collect_metrics: true
  collect_usage: true

# Webhook Settings
webhook:
  url: "os.environ/LITELLM_WEBHOOK_URL"
  events: "os.environ/LITELLM_WEBHOOK_EVENTS"

# Advanced Settings
advanced_settings:
  # Production request timeout (increased for stability)
  request_timeout: 600
  
  # Reduced concurrent requests for memory efficiency
  max_concurrent_requests: 50
  
  # Minimal logging for production
  log_requests: false
  log_responses: false
  
  # Enable cost tracking
  cost_tracking: true
  
  # Production worker settings
  num_workers: 1 
